{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ensemble-classifier-combination-tester.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMzi1ZK2vtSV/MiVZn/t4wA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acevedosharp/ensemble-testing-chamber/blob/master/notebooks/ensemble_classifier_combination_tester.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5POcuMGvFbr"
      },
      "source": [
        "# Measuring the effectiveness of ensembles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VVkWfGqHHs6"
      },
      "source": [
        "- assemble all possible combination of ensembles $k \\in \\{1,2,3,4,5\\}$\r\n",
        "- 10-fold cross validation\r\n",
        "- the single best learner (according to some cross-validation) for each dataset\r\n",
        "- for each $k \\in \\{2,3,4,5\\}$, how many ensembles do improve over the single best learner by at least $0.005$ (absolute and relative counts of improvement)\r\n",
        "- for each base learner understand in how many improving ensembles it is contained (absolute and relative numbers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCu4pVZQvAQq"
      },
      "source": [
        "## Experimental setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOTCnQy8CnV0"
      },
      "source": [
        "import itertools\r\n",
        "import random\r\n",
        "import openml\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import sklearn\r\n",
        "from datetime import datetime\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.impute import KNNImputer\r\n",
        "\r\n",
        "# classifiers\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.tree import ExtraTreeClassifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\r\n",
        "from sklearn.linear_model import Perceptron\r\n",
        "from sklearn.linear_model import RidgeClassifier\r\n",
        "from sklearn.linear_model import SGDClassifier\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\r\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\r\n",
        "from sklearn.naive_bayes import BernoulliNB\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.ensemble import ExtraTreesClassifier\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.ensemble import GradientBoostingClassifier\r\n",
        "\r\n",
        "from datetime import datetime"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEHheHDUezaf"
      },
      "source": [
        "allDatasets = [\r\n",
        "  (1485 , 'madelon'),\r\n",
        "  # (42734, 'okcupid-stem'), requires too much preprocessing\r\n",
        "  (1169 , 'airlines'),\r\n",
        "  (42733, 'Click_prediction_small'),\r\n",
        "  (41150, 'MiniBooNE'),\r\n",
        "  (54   , 'vehicle'),\r\n",
        "  (40981, 'Australian'),\r\n",
        "  (40975, 'car'),\r\n",
        "  (40670, 'dna'),\r\n",
        "  (31   , 'credit-g'),\r\n",
        "  (41169, 'helena'),\r\n",
        "  (41168, 'jannis'),\r\n",
        "  (41167, 'dionis'),\r\n",
        "  (41166, 'volkert'),\r\n",
        "  (41165, 'robert'),\r\n",
        "  (41164, 'fabert'),\r\n",
        "  (41163, 'dilbert'),\r\n",
        "  (41162, 'kick'),\r\n",
        "  (41161, 'riccardo'),\r\n",
        "  (41159, 'guillermo'),\r\n",
        "]\r\n",
        "\r\n",
        "indicesToRun = [0] #list(range(len(allDatasets)))\r\n",
        "rawDatasets = { allDatasets[i][1]: openml.datasets.get_dataset(allDatasets[i][0]) for i in indicesToRun } # download the datasets that will actually be used for the experiment\r\n",
        "\r\n",
        "datasets = { }\r\n",
        "\r\n",
        "for datasetName, rawDataset in rawDatasets.items():\r\n",
        "  X, Y, categorical_indicator, attribute_names = rawDatasets[datasetName].get_data(\r\n",
        "    dataset_format=\"dataframe\", target=rawDatasets[datasetName].default_target_attribute\r\n",
        "  )\r\n",
        "  X = pd.get_dummies(X).to_numpy()\r\n",
        "  Y = Y.to_numpy()\r\n",
        "  if X.shape[0] > 25000: # max num instances is 25k, more and it takes an eternity or it doesn't fit in memory when too many attributes\r\n",
        "    arr = np.arange(0, X.shape[0])\r\n",
        "    random.shuffle(arr)\r\n",
        "    X = np.take(X, arr[:25000], axis=0)\r\n",
        "    Y = np.take(Y, arr[:25000], axis=0)\r\n",
        "\r\n",
        "  datasets[datasetName] = (\r\n",
        "      X,\r\n",
        "      LabelEncoder().fit_transform(Y), # make sure all clases are integers\r\n",
        "      categorical_indicator\r\n",
        "  )"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOeNKRHjdcR8"
      },
      "source": [
        "classifiers = {\r\n",
        "    \"Linear SVC\": LinearSVC(),\r\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\r\n",
        "    \"Extra Tree\": ExtraTreeClassifier(),\r\n",
        "    \"Logistic\": LogisticRegression(),\r\n",
        "    \"Passive Aggressive\": PassiveAggressiveClassifier(),\r\n",
        "    \"Perceptron\": Perceptron(),\r\n",
        "    \"Ridge\": RidgeClassifier(),\r\n",
        "    \"SGD\": SGDClassifier(),\r\n",
        "    \"Multi-layer Perceptron\": MLPClassifier(),\r\n",
        "    \"Linear Discriminant\": LinearDiscriminantAnalysis(),\r\n",
        "    \"Quadratic Discriminant\": QuadraticDiscriminantAnalysis(),\r\n",
        "    \"BernoulliNB\": BernoulliNB(),\r\n",
        "    \"MultinomialNB\": MultinomialNB(),\r\n",
        "    \"Nearest Neighbors\": KNeighborsClassifier(),\r\n",
        "    \"Extra Trees\": ExtraTreesClassifier(),\r\n",
        "    \"Random Forest (10 estimators)\": RandomForestClassifier(n_estimators=10),\r\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier()\r\n",
        "}"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5YLMIyYkxk2"
      },
      "source": [
        "### Execute experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4ORrJknmQBF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946c8a79-21fc-43e3-a058-e3338f799fba"
      },
      "source": [
        "# 10-fold cross validation, take only 25k instances for training in dataset if larger\r\n",
        "FOLDS = 10\r\n",
        "\r\n",
        "results = []\r\n",
        "singleClassifiersRuntimeAcrossDifferentFolds = [{classifier: (datetime.now() - datetime.now()) for classifier in classifiers.keys()} for i in range(FOLDS)]\r\n",
        "\r\n",
        "kf = KFold(n_splits=FOLDS)\r\n",
        "for dataset in datasets.keys():\r\n",
        "  X, Y = datasets[dataset][0], datasets[dataset][1]\r\n",
        "  #X = StandardScaler().fit_transform(X)\r\n",
        "  fold_index = 0\r\n",
        "  for train_index, test_index in kf.split(X):\r\n",
        "    fold_start_time = datetime.now()\r\n",
        "    print(\">>> Fold\", fold_index)\r\n",
        "    X_train, X_test = X[train_index], X[test_index] # np.take\r\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index] # np.take\r\n",
        "\r\n",
        "    # Train every classifier with the new data\r\n",
        "    trainStartTime = datetime.now()\r\n",
        "    for classifier_name, classifier in classifiers.items():\r\n",
        "      print(f\"Training {classifier_name}\", end='')\r\n",
        "      classifierStartTime = datetime.now()\r\n",
        "      classifier.fit(X_train, Y_train)\r\n",
        "      delta = datetime.now() - classifierStartTime\r\n",
        "      print(f'({datetime.now() - classifierStartTime})')\r\n",
        "      singleClassifiersRuntimeAcrossDifferentFolds[fold_index][classifier_name] = delta\r\n",
        "    print(f'Training all classifiers on data took {datetime.now() - trainStartTime}')\r\n",
        "\r\n",
        "    # Assemble ensembles of size k in {1,2,3,4,5}\r\n",
        "    for k in range(1,6):\r\n",
        "      print(\"\\t>>> k:\", k)\r\n",
        "      kStartTime = datetime.now()\r\n",
        "      for combination in list(itertools.combinations(classifiers.keys(), k)):\r\n",
        "        singleEnsembleStartTime = datetime.now()\r\n",
        "        ensemble = []\r\n",
        "        ensemble_description = \"\"\r\n",
        "\r\n",
        "        # group classifiers (already exist fitted in dict)\r\n",
        "        for idx in range(k):\r\n",
        "          ensemble.append(classifiers[combination[idx]])\r\n",
        "          ensemble_description += combination[idx]\r\n",
        "          ensemble_description += \"-\"\r\n",
        "        ensemble_description = ensemble_description[:-1]\r\n",
        "\r\n",
        "        # save predictions\r\n",
        "        predictions = np.zeros((len(X_test), k)) # (# test instances, ensemble size)\r\n",
        "        for idx, classifier in enumerate(ensemble):\r\n",
        "          predictions[:,idx] = classifier.predict(X_test)\r\n",
        "        \r\n",
        "        # do hard voting\r\n",
        "        hard_voting_predictions = np.zeros((len(X_test), 1)) # (# test instances, 1)\r\n",
        "        for idx in range(predictions.shape[0]):\r\n",
        "          values, counts = np.unique(predictions[idx], return_counts=True)\r\n",
        "          hard_voting_predictions[idx] = values[np.argmax(counts)]\r\n",
        "        \r\n",
        "        # compare voting predictions against Y_test\r\n",
        "        total_instance_number = len(X_test)\r\n",
        "        errors = 0\r\n",
        "        for idx in range(hard_voting_predictions.shape[0]):\r\n",
        "          if (hard_voting_predictions[idx][0] != Y_test[idx]):\r\n",
        "            errors += 1\r\n",
        "        score = errors/total_instance_number\r\n",
        "\r\n",
        "        # save result (ensemble description, ensemble size, fold index, dataset, score)\r\n",
        "        results.append((ensemble_description, k, fold_index, dataset, score, datetime.now() - singleEnsembleStartTime))\r\n",
        "      print(f\"k took {datetime.now() - kStartTime}\")\r\n",
        "\r\n",
        "    fold_index += 1\r\n",
        "    print(\">>> fold took:\", datetime.now() - fold_start_time)\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Fold 0\n",
            "Training Linear SVC"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\aceve\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(0:00:03.294002)\n",
            "Training Decision Tree(0:00:00.841995)\n",
            "Training Extra Tree(0:00:00.015998)\n",
            "Training Logistic"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\aceve\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(0:00:00.223003)\n",
            "Training Passive Aggressive(0:00:00.085996)\n",
            "Training Perceptron(0:00:00.032001)\n",
            "Training Ridge(0:00:00.036007)\n",
            "Training SGD(0:00:00.364985)\n",
            "Training Multi-layer Perceptron(0:00:00.664008)\n",
            "Training Linear Discriminant(0:00:00.305008)\n",
            "Training Quadratic Discriminant(0:00:00.215987)\n",
            "Training BernoulliNB(0:00:00.016008)\n",
            "Training MultinomialNB(0:00:00.006994)\n",
            "Training Nearest Neighbors(0:00:00.013999)\n",
            "Training Extra Trees(0:00:01.290002)\n",
            "Training Random Forest (10 estimators)(0:00:00.259994)\n",
            "Training Gradient Boosting(0:00:18.886988)\n",
            "Training all classifiers on data took 0:00:26.561004\n",
            "\t>>> k: 1\n",
            "k took 0:00:00.201994\n",
            "\t>>> k: 2\n",
            "k took 0:00:02.310004\n",
            "\t>>> k: 3\n",
            "k took 0:00:14.744508\n",
            "\t>>> k: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju1w2RGxk1lc"
      },
      "source": [
        "## Analysing results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scSXWuWTlEVx"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vVQX6L1lKVd"
      },
      "source": [
        "def saveResultsAsCsv(filename, headers, rows):\r\n",
        "  if len(headers) == len(rows[0]):\r\n",
        "    with open(filename,'w') as file:\r\n",
        "      file.write(','.join(headers))\r\n",
        "      file.write('\\n')\r\n",
        "      for row in rows:\r\n",
        "        file.write(','.join(list(map(lambda rc: str(rc), row))))\r\n",
        "        file.write('\\n')\r\n",
        "  else:\r\n",
        "    raise Exception('length of headers does not match length of single rows.')\r\n",
        "\r\n",
        "def filterResultsByDataset(datasetName, results):\r\n",
        "  return list(filter(lambda res: str(res[2]) == datasetName, results))\r\n",
        "\r\n",
        "def loadResultsFromCsv(filename):\r\n",
        "  loadedResults = []\r\n",
        "  with open(filename) as file:\r\n",
        "    loadedResults = list(map(lambda entry: entry.split(','), [x.strip() for x in file.readlines()]))\r\n",
        "  file.close()\r\n",
        "  print('Headers are:', loadedResults[0])\r\n",
        "  del loadedResults[0]\r\n",
        "  return loadedResults\r\n"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFYgGR4blnTo"
      },
      "source": [
        "### Aggregate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBZs0JFQbbft"
      },
      "source": [
        "uniqueEnsembles = list(itertools.combinations(classifiers.keys(), k))\r\n",
        "finalResults = []\r\n",
        "\r\n",
        "for i, ensemble_description in enumerate(uniqueEnsembles):\r\n",
        "  filtered = list(filter(lambda res: str(res[0]) == ensemble_description, results))\r\n",
        "  sum = 0\r\n",
        "  for entry in filtered:\r\n",
        "    sum += float(entry[4])\r\n",
        "  mean_error = sum / 5\r\n",
        "  finalResults.append((filtered[0][0], filtered[0][1], filtered[0][3], mean_error)) # (ensemble_description, ensemble_size, dataset, mean_error)\r\n"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9QYgGRnPfgT"
      },
      "source": [
        "finalResults = loadResultsFromCsv('finalResults.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSMHbcE-rJRd"
      },
      "source": [
        "### Save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rikxkjMrOFX"
      },
      "source": [
        "saveResultsAsCsv('finalResults.csv', ['ensembleDescription', 'k', 'dataset', 'errorRate'], finalResults)"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdnGpkoRT2tG"
      },
      "source": [
        "### Load results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA_J9oNNT4qH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c1bd19-dadb-4252-c821-7be4691e159d"
      },
      "source": [
        "finalResults = loadResultsFromCsv('finalResults.csv')\r\n",
        "resultsDf = pd.read_csv('finalResults.csv')"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Headers are: ['ensembleDescription', 'k', 'dataset', 'errorRate']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nL_M4UJqSog"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17hLSwlgqcQw"
      },
      "source": [
        "#### Error rates for different sizes of k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVysIdAcpB-Y"
      },
      "source": [
        "for dataset, matchingResults in resultsDf.groupby(['dataset']): \r\n",
        "  boxplots = []\r\n",
        "  for k, matchingKs in matchingResults.groupby(['k']):\r\n",
        "    boxplots.append(matchingKs['errorRate'].values)\r\n",
        "  fig, ax = plt.subplots()\r\n",
        "  ax.boxplot(boxplots)\r\n",
        "  ax.set_title(dataset)\r\n",
        "  plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_54zvHabpJp0"
      },
      "source": [
        "#### Best single learners for every dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vTSoKttpN1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5afa6bb7-2e78-47d2-d056-f86b3aba0146"
      },
      "source": [
        "bestSingleLearners = {} # dataset: (ensembleDescription, errorRate)\r\n",
        "\r\n",
        "for dataset, matchingResults in resultsDf.groupby(['dataset']): \r\n",
        "  matchingK1s = matchingResults.loc[resultsDf['k'] == 1]\r\n",
        "  minIndex = matchingK1s['errorRate'].idxmin()\r\n",
        "  minRow = resultsDf.iloc[minIndex]\r\n",
        "  bestSingleLearners[dataset] = (minRow['ensembleDescription'], minRow['errorRate'])\r\n",
        "\r\n",
        "bestSingleLearners"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'madelon': ('Linear SVC', 1.0)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--KHaeYhq5lm"
      },
      "source": [
        "#### For every $k \\in \\{2,3,4,5\\}$, how many ensembles do improve over the single best learner by at least $0.005$ (absolute and relative counts of improvement)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXxF7SdyvCHd"
      },
      "source": [
        "improvementMargin = 0.005\r\n",
        "\r\n",
        "improvementEnsembles = {dataset: [] for dataset in bestSingleLearners.keys()} # dataset: (ensembleDescription, errorRate, k)\r\n",
        "\r\n",
        "for dataset, matchingResults in resultsDf.groupby(['dataset']): \r\n",
        "  for k, matchingKs in matchingResults.groupby(['k']):\r\n",
        "    if k != 1: # ...\r\n",
        "      for i, row in matchingKs.iterrows():\r\n",
        "        if (row['errorRate'] + improvementMargin) < bestSingleLearners[dataset][1]:\r\n",
        "          improvementEnsembles[dataset].append((row['ensembleDescription'], row['errorRate'], k))"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhWhd6OyygLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ff5d2f-7f85-4e17-f9de-15d7568bd293"
      },
      "source": [
        "for dataset in improvementEnsembles.keys():\r\n",
        "  print(f\"\\n{dataset}'s best single learner had an error rate of: {bestSingleLearners[dataset][1]}\")\r\n",
        "  nImproved = len(improvementEnsembles[dataset])\r\n",
        "  print(f\"\\t{nImproved} ({round(nImproved/resultsDf[(resultsDf['dataset'] == dataset) & (resultsDf['k'] != 1)]['dataset'].count()*100, 2)}%) ensembles improved by at least {improvementMargin}\")\r\n",
        "  for k in range(2, 6):\r\n",
        "    matchingImprovementEnsemblesWithK = list(filter(lambda entry: entry[2] == k, improvementEnsembles[dataset]))\r\n",
        "    print(f\"\\t\\t{len(matchingImprovementEnsemblesWithK)} with k={k} - mean error rate: {np.mean(list(map(lambda tup: tup[1], matchingImprovementEnsemblesWithK)))}\")"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "madelon's best single learner had an error rate of: 1.0\n",
            "\t0 (0.0%) ensembles improved by at least 0.005\n",
            "\t\t0 with k=2 - mean error rate: nan\n",
            "\t\t0 with k=3 - mean error rate: nan\n",
            "\t\t0 with k=4 - mean error rate: nan\n",
            "\t\t0 with k=5 - mean error rate: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\aceve\\miniconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "C:\\Users\\aceve\\miniconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akz8ZeAD3TeB"
      },
      "source": [
        "#### For each base learner understand in how many improving ensembles it is contained (absolute and relative numbers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix4Xqxme34Ym"
      },
      "source": [
        "for dataset in ds_names:\r\n",
        "  print(f'Dataset: {dataset}')\r\n",
        "  improvers = [] # classifier, timesAppearedInImprovements, timesAppearedInImprovementsRelative\r\n",
        "  for classifier in classifiers.keys():\r\n",
        "    timesAppearedInImprovements = 0\r\n",
        "    for row in improvementEnsembles[dataset]:\r\n",
        "      if classifier in row[0]:\r\n",
        "        timesAppearedInImprovements += 1\r\n",
        "    if len(improvementEnsembles[dataset]) != 0:\r\n",
        "      improvers.append((classifier, timesAppearedInImprovements, timesAppearedInImprovements/len(improvementEnsembles[dataset])))\r\n",
        "    else:\r\n",
        "      improvers.append((classifier, 0, 0))\r\n",
        "\r\n",
        "  for entry in sorted(improvers, key=lambda tup: tup[1], reverse=True):\r\n",
        "    print(f\"\\t{entry[0]}: {entry[1]} ({entry[2]*100}%)\")\r\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}