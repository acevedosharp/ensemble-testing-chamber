{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ensemble-classifier-combination-tester.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOAPyL9XD0NiLUXmLCVzxxQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acevedosharp/ensemble-testing-chamber/blob/master/notebooks/ensemble_classifier_combination_tester.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5POcuMGvFbr"
      },
      "source": [
        "# Measuring the effectiveness of ensembles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VVkWfGqHHs6"
      },
      "source": [
        "- assemble all possible combination of ensembles $k \\in \\{1,2,3,4,5\\}$\r\n",
        "- 10-fold cross validation\r\n",
        "- the single best learner (according to some cross-validation) for each dataset\r\n",
        "- for each $k \\in \\{2,3,4,5\\}$, how many ensembles do improve over the single best learner by at least $0.005$ (absolute and relative counts of improvement)\r\n",
        "- for each base learner understand in how many improving ensembles it is contained (absolute and relative numbers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCu4pVZQvAQq"
      },
      "source": [
        "## Experimental setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOTCnQy8CnV0"
      },
      "source": [
        "# ignore sklearn warnings\r\n",
        "def warn(*args, **kwargs):\r\n",
        "    pass\r\n",
        "import warnings\r\n",
        "warnings.warn = warn\r\n",
        "\r\n",
        "import itertools\r\n",
        "import random\r\n",
        "import openml\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import sklearn\r\n",
        "from datetime import datetime\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from functools import reduce\r\n",
        "\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.impute import KNNImputer\r\n",
        "\r\n",
        "# classifiers\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.tree import ExtraTreeClassifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\r\n",
        "from sklearn.linear_model import Perceptron\r\n",
        "from sklearn.linear_model import RidgeClassifier\r\n",
        "from sklearn.linear_model import SGDClassifier\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\r\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\r\n",
        "from sklearn.naive_bayes import BernoulliNB\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.ensemble import ExtraTreesClassifier\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.ensemble import GradientBoostingClassifier\r\n",
        "\r\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEHheHDUezaf"
      },
      "source": [
        "allDatasets = [\r\n",
        "  (1485 , 'madelon'),\r\n",
        "  # (42734, 'okcupid-stem'), requires too much preprocessing\r\n",
        "  (1169 , 'airlines'),\r\n",
        "  # (42733, 'Click_prediction_small'), after creating dummy columns it becomes way too large to fit on machines with less than 32gb of memory.\r\n",
        "  (41150, 'MiniBooNE'),\r\n",
        "  (54   , 'vehicle'), \r\n",
        "  (40981, 'Australian'),\r\n",
        "  (40975, 'car'),\r\n",
        "  (40670, 'dna'),\r\n",
        "  (31   , 'credit-g'),\r\n",
        "  (41169, 'helena'),\r\n",
        "  (41168, 'jannis'),\r\n",
        "  (41167, 'di  nis'),\r\n",
        "  (41166, 'volkert'),\r\n",
        "  (41165, 'robert'),\r\n",
        "  (41164, 'fabert'),\r\n",
        "  (41163, 'dilbert'),\r\n",
        "  (41162, 'kick'),\r\n",
        "  (41161, 'riccardo'),\r\n",
        "  (41159, 'guillermo'),\r\n",
        "]\r\n",
        "\r\n",
        "indicesToRun = [9] # list(range(len(allDatasets)))\r\n",
        "rawDatasets = { allDatasets[i][1]: openml.datasets.get_dataset(allDatasets[i][0]) for i in indicesToRun } # download the datasets that will actually be used for the experiment\r\n",
        "\r\n",
        "datasets = { }\r\n",
        "\r\n",
        "for datasetName, rawDataset in rawDatasets.items():\r\n",
        "  X, Y, categorical_indicator, attribute_names = rawDatasets[datasetName].get_data(\r\n",
        "    dataset_format=\"dataframe\", target=rawDatasets[datasetName].default_target_attribute\r\n",
        "  )\r\n",
        "  if X.shape[0] > 25000: \r\n",
        "    arr = np.arange(0, X.shape[0])\r\n",
        "    random.shuffle(arr)\r\n",
        "    X = X.iloc[arr[:25000]]\r\n",
        "    Y = Y.iloc[arr[:25000]]\r\n",
        "  \r\n",
        "  X = pd.get_dummies(X).to_numpy()\r\n",
        "  Y = Y.to_numpy()\r\n",
        "  \r\n",
        "  datasets[datasetName] = (\r\n",
        "      X,\r\n",
        "      LabelEncoder().fit_transform(Y), # make sure all clases are integers\r\n",
        "      categorical_indicator\r\n",
        "  )\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAyRvsAVKPLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71592f78-de4b-4660-f6f7-d0cc66d3e2df"
      },
      "source": [
        "datasets.keys()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['jannis'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7b6pFVrlyPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec5791a-2a5a-4c45-9e4c-b93960f225eb"
      },
      "source": [
        "datasets['jannis'][0].shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 54)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOeNKRHjdcR8"
      },
      "source": [
        "classifiers = {\r\n",
        "    \"Linear SVC\": LinearSVC(),\r\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\r\n",
        "    \"Extra Tree\": ExtraTreeClassifier(),\r\n",
        "    \"Logistic\": LogisticRegression(),\r\n",
        "    \"Passive Aggressive\": PassiveAggressiveClassifier(),\r\n",
        "    \"Perceptron\": Perceptron(),\r\n",
        "    \"Ridge\": RidgeClassifier(),\r\n",
        "    \"SGD\": SGDClassifier(),\r\n",
        "    \"Multi-layer Perceptron\": MLPClassifier(),\r\n",
        "    \"Linear Discriminant\": LinearDiscriminantAnalysis(),\r\n",
        "    \"Quadratic Discriminant\": QuadraticDiscriminantAnalysis(),\r\n",
        "    \"BernoulliNB\": BernoulliNB(),\r\n",
        "    # don't forget to comment it when dataset contains negative values\r\n",
        "    #\"MultinomialNB\": MultinomialNB(),\r\n",
        "    \"Nearest Neighbors\": KNeighborsClassifier(),\r\n",
        "    \"Extra Trees\": ExtraTreesClassifier(),\r\n",
        "    \"Random Forest (10 estimators)\": RandomForestClassifier(n_estimators=10),\r\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier()\r\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5YLMIyYkxk2"
      },
      "source": [
        "### Execute experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4ORrJknmQBF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1752206-d079-4a3a-b7d8-3cda29e9a37a"
      },
      "source": [
        "# 10-fold cross validation, take only 25k instances for training in dataset if larger\r\n",
        "FOLDS = 10\r\n",
        "\r\n",
        "results = []\r\n",
        "singleClassifiersRuntimeAcrossDifferentFolds = [{classifier: (datetime.now() - datetime.now()) for classifier in classifiers.keys()} for i in range(FOLDS)]\r\n",
        "\r\n",
        "kf = KFold(n_splits=FOLDS)\r\n",
        "for dataset in datasets.keys():\r\n",
        "  X, Y = datasets[dataset][0], datasets[dataset][1]\r\n",
        "  #X = StandardScaler().fit_transform(X)\r\n",
        "  fold_index = 0\r\n",
        "  for train_index, test_index in kf.split(X):\r\n",
        "    fold_start_time = datetime.now()\r\n",
        "    print(\">>> Fold\", fold_index)\r\n",
        "    X_train, X_test = X[train_index], X[test_index] # np.take\r\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index] # np.take\r\n",
        "\r\n",
        "    # Train every classifier with the new data\r\n",
        "    trainStartTime = datetime.now()\r\n",
        "    for classifier_name, classifier in classifiers.items():\r\n",
        "      print(f\"Training {classifier_name}\", end='')\r\n",
        "      classifierStartTime = datetime.now()\r\n",
        "      classifier.fit(X_train, Y_train)\r\n",
        "      delta = datetime.now() - classifierStartTime\r\n",
        "      print(f'({datetime.now() - classifierStartTime})')\r\n",
        "      singleClassifiersRuntimeAcrossDifferentFolds[fold_index][classifier_name] = delta\r\n",
        "    print(f'Training all classifiers on data took {datetime.now() - trainStartTime}')\r\n",
        "\r\n",
        "    # Assemble ensembles of size k in {1,2,3,4,5}\r\n",
        "    for k in range(1,6):\r\n",
        "      print(\"\\t>>> k:\", k)\r\n",
        "      kStartTime = datetime.now()\r\n",
        "      for combination in list(itertools.combinations(classifiers.keys(), k)):\r\n",
        "        singleEnsembleStartTime = datetime.now()\r\n",
        "        ensemble = []\r\n",
        "        ensemble_description = \"\"\r\n",
        "\r\n",
        "        # group classifiers (already exist fitted in dict)\r\n",
        "        for idx in range(k):\r\n",
        "          ensemble.append(classifiers[combination[idx]])\r\n",
        "          ensemble_description += combination[idx]\r\n",
        "          ensemble_description += \"-\"\r\n",
        "        ensemble_description = ensemble_description[:-1]\r\n",
        "\r\n",
        "        # save predictions\r\n",
        "        predictions = np.zeros((len(X_test), k)) # (# test instances, ensemble size)\r\n",
        "        for idx, classifier in enumerate(ensemble):\r\n",
        "          predictions[:,idx] = classifier.predict(X_test)\r\n",
        "        \r\n",
        "        # do hard voting\r\n",
        "        hard_voting_predictions = np.zeros((len(X_test), 1)) # (# test instances, 1)\r\n",
        "        for idx in range(predictions.shape[0]):\r\n",
        "          values, counts = np.unique(predictions[idx], return_counts=True)\r\n",
        "          hard_voting_predictions[idx] = values[np.argmax(counts)]\r\n",
        "        \r\n",
        "        # compare voting predictions against Y_test\r\n",
        "        total_instance_number = len(X_test)\r\n",
        "        errors = 0\r\n",
        "        for idx in range(hard_voting_predictions.shape[0]):\r\n",
        "          if (hard_voting_predictions[idx][0] != Y_test[idx]):\r\n",
        "            errors += 1\r\n",
        "        score = errors/total_instance_number\r\n",
        "\r\n",
        "        # save result (ensemble description, ensemble size, fold index, dataset, score, timedelta)\r\n",
        "        results.append((ensemble_description, k, fold_index, dataset, score, datetime.now() - singleEnsembleStartTime))\r\n",
        "      print(f\"k took {datetime.now() - kStartTime}\")\r\n",
        "\r\n",
        "    fold_index += 1\r\n",
        "    print(\">>> fold took:\", datetime.now() - fold_start_time)\r\n",
        "        "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Fold 0\n",
            "Training Linear SVC(0:00:24.170993)\n",
            "Training Decision Tree(0:00:01.918041)\n",
            "Training Extra Tree(0:00:00.053987)\n",
            "Training Logistic(0:00:01.233976)\n",
            "Training Passive Aggressive(0:00:00.374026)\n",
            "Training Perceptron(0:00:00.267001)\n",
            "Training Ridge(0:00:00.028967)\n",
            "Training SGD(0:00:04.922002)\n",
            "Training Multi-layer Perceptron(0:00:05.597999)\n",
            "Training Linear Discriminant(0:00:00.160001)\n",
            "Training Quadratic Discriminant(0:00:00.055002)\n",
            "Training BernoulliNB(0:00:00.021000)\n",
            "Training Nearest Neighbors(0:00:00.002999)\n",
            "Training Extra Trees(0:00:04.194038)\n",
            "Training Random Forest (10 estimators)(0:00:01.427007)\n",
            "Training Gradient Boosting(0:02:31.877010)\n",
            "Training all classifiers on data took 0:03:16.306045\n",
            "\t>>> k: 1\n",
            "k took 0:00:02.368987\n",
            "\t>>> k: 2\n",
            "k took 0:00:32.627997\n",
            "\t>>> k: 3\n",
            "k took 0:03:30.007000\n",
            "\t>>> k: 4\n",
            "k took 0:13:09.473898\n",
            "\t>>> k: 5\n",
            "k took 0:39:39.209207\n",
            ">>> fold took: 1:00:10.004152\n",
            ">>> Fold 1\n",
            "Training Linear SVC(0:00:24.579053)\n",
            "Training Decision Tree(0:00:01.798023)\n",
            "Training Extra Tree(0:00:00.046005)\n",
            "Training Logistic(0:00:00.941947)\n",
            "Training Passive Aggressive(0:00:00.340999)\n",
            "Training Perceptron(0:00:00.312029)\n",
            "Training Ridge(0:00:00.030972)\n",
            "Training SGD(0:00:05.051000)\n",
            "Training Multi-layer Perceptron(0:00:07.643577)\n",
            "Training Linear Discriminant(0:00:00.180974)\n",
            "Training Quadratic Discriminant(0:00:00.050998)\n",
            "Training BernoulliNB(0:00:00.019005)\n",
            "Training Nearest Neighbors(0:00:00.003954)\n",
            "Training Extra Trees(0:00:04.221027)\n",
            "Training Random Forest (10 estimators)(0:00:01.427025)\n",
            "Training Gradient Boosting(0:03:01.050012)\n",
            "Training all classifiers on data took 0:03:47.703611\n",
            "\t>>> k: 1\n",
            "k took 0:00:02.759995\n",
            "\t>>> k: 2\n",
            "k took 0:00:32.095731\n",
            "\t>>> k: 3\n",
            "k took 0:03:23.268789\n",
            "\t>>> k: 4\n",
            "k took 0:13:41.991295\n",
            "\t>>> k: 5\n",
            "k took 0:40:33.491612\n",
            ">>> fold took: 1:02:01.332999\n",
            ">>> Fold 2\n",
            "Training Linear SVC(0:00:35.373433)\n",
            "Training Decision Tree(0:00:02.103481)\n",
            "Training Extra Tree(0:00:00.073186)\n",
            "Training Logistic(0:00:01.125134)\n",
            "Training Passive Aggressive(0:00:00.418694)\n",
            "Training Perceptron(0:00:00.413949)\n",
            "Training Ridge(0:00:00.040161)\n",
            "Training SGD(0:00:06.121684)\n",
            "Training Multi-layer Perceptron(0:00:08.336131)\n",
            "Training Linear Discriminant(0:00:00.224482)\n",
            "Training Quadratic Discriminant(0:00:00.057001)\n",
            "Training BernoulliNB(0:00:00.023001)\n",
            "Training Nearest Neighbors(0:00:00.005000)\n",
            "Training Extra Trees(0:00:05.043090)\n",
            "Training Random Forest (10 estimators)(0:00:01.665277)\n",
            "Training Gradient Boosting(0:02:39.862803)\n",
            "Training all classifiers on data took 0:03:40.890492\n",
            "\t>>> k: 1\n",
            "k took 0:00:02.660735\n",
            "\t>>> k: 2\n",
            "k took 0:00:30.978752\n",
            "\t>>> k: 3\n",
            "k took 0:03:02.562152\n",
            "\t>>> k: 4\n",
            "k took 0:12:12.401297\n",
            "\t>>> k: 5\n",
            "k took 0:38:58.497349\n",
            ">>> fold took: 0:58:28.014780\n",
            ">>> Fold 3\n",
            "Training Linear SVC(0:00:33.346468)\n",
            "Training Decision Tree(0:00:01.980041)\n",
            "Training Extra Tree(0:00:00.050971)\n",
            "Training Logistic(0:00:01.014072)\n",
            "Training Passive Aggressive(0:00:00.403000)\n",
            "Training Perceptron(0:00:00.295008)\n",
            "Training Ridge(0:00:00.031992)\n",
            "Training SGD(0:00:05.248970)\n",
            "Training Multi-layer Perceptron(0:00:12.511045)\n",
            "Training Linear Discriminant(0:00:00.195997)\n",
            "Training Quadratic Discriminant(0:00:00.058003)\n",
            "Training BernoulliNB(0:00:00.026001)\n",
            "Training Nearest Neighbors(0:00:00.005001)\n",
            "Training Extra Trees(0:00:04.931997)\n",
            "Training Random Forest (10 estimators)(0:00:01.563002)\n",
            "Training Gradient Boosting(0:02:51.731548)\n",
            "Training all classifiers on data took 0:03:53.399168\n",
            "\t>>> k: 1\n",
            "k took 0:00:02.682994\n",
            "\t>>> k: 2\n",
            "k took 0:00:34.024190\n",
            "\t>>> k: 3\n",
            "k took 0:03:30.053442\n",
            "\t>>> k: 4\n",
            "k took 0:13:59.486246\n",
            "\t>>> k: 5\n",
            "k took 0:37:53.057860\n",
            ">>> fold took: 0:59:52.729903\n",
            ">>> Fold 4\n",
            "Training Linear SVC(0:00:24.279036)\n",
            "Training Decision Tree(0:00:01.878963)\n",
            "Training Extra Tree(0:00:00.044001)\n",
            "Training Logistic(0:00:01.176002)\n",
            "Training Passive Aggressive(0:00:00.308002)\n",
            "Training Perceptron(0:00:00.281000)\n",
            "Training Ridge(0:00:00.032999)\n",
            "Training SGD(0:00:05.129001)\n",
            "Training Multi-layer Perceptron(0:00:06.589998)\n",
            "Training Linear Discriminant(0:00:00.185002)\n",
            "Training Quadratic Discriminant(0:00:00.056996)\n",
            "Training BernoulliNB(0:00:00.025000)\n",
            "Training Nearest Neighbors(0:00:00.006000)\n",
            "Training Extra Trees(0:00:04.314004)\n",
            "Training Random Forest (10 estimators)(0:00:01.388034)\n",
            "Training Gradient Boosting(0:02:42.818104)\n",
            "Training all classifiers on data took 0:03:28.514143\n",
            "\t>>> k: 1\n",
            "k took 0:00:02.777993\n",
            "\t>>> k: 2\n",
            "k took 0:00:32.529999\n",
            "\t>>> k: 3\n",
            "k took 0:03:24.763036\n",
            "\t>>> k: 4\n",
            "k took 0:13:50.222650\n",
            "\t>>> k: 5\n",
            "k took 0:40:40.951856\n",
            ">>> fold took: 1:01:59.781715\n",
            ">>> Fold 5\n",
            "Training Linear SVC(0:00:36.360135)\n",
            "Training Decision Tree(0:00:01.946437)\n",
            "Training Extra Tree(0:00:00.050447)\n",
            "Training Logistic(0:00:00.902754)\n",
            "Training Passive Aggressive(0:00:00.394579)\n",
            "Training Perceptron(0:00:00.236004)\n",
            "Training Ridge(0:00:00.031887)\n",
            "Training SGD(0:00:05.560730)\n",
            "Training Multi-layer Perceptron(0:00:09.457575)\n",
            "Training Linear Discriminant(0:00:00.242004)\n",
            "Training Quadratic Discriminant(0:00:00.061000)\n",
            "Training BernoulliNB(0:00:00.026999)\n",
            "Training Nearest Neighbors(0:00:00.005999)\n",
            "Training Extra Trees(0:00:05.154845)\n",
            "Training Random Forest (10 estimators)(0:00:01.573608)\n",
            "Training Gradient Boosting(0:02:55.895797)\n",
            "Training all classifiers on data took 0:03:57.907732\n",
            "\t>>> k: 1\n",
            "k took 0:00:03.707752\n",
            "\t>>> k: 2\n",
            "k took 0:00:43.859398\n",
            "\t>>> k: 3\n",
            "k took 0:03:50.673757\n",
            "\t>>> k: 4\n",
            "k took 0:13:57.606744\n",
            "\t>>> k: 5\n",
            "k took 0:38:39.136786\n",
            ">>> fold took: 1:01:12.915275\n",
            ">>> Fold 6\n",
            "Training Linear SVC(0:00:26.973748)\n",
            "Training Decision Tree(0:00:01.868315)\n",
            "Training Extra Tree(0:00:00.043999)\n",
            "Training Logistic(0:00:01.124035)\n",
            "Training Passive Aggressive(0:00:00.343230)\n",
            "Training Perceptron(0:00:00.337108)\n",
            "Training Ridge(0:00:00.029118)\n",
            "Training SGD(0:00:05.414679)\n",
            "Training Multi-layer Perceptron(0:00:08.080839)\n",
            "Training Linear Discriminant(0:00:00.182030)\n",
            "Training Quadratic Discriminant(0:00:00.065001)\n",
            "Training BernoulliNB(0:00:00.027000)\n",
            "Training Nearest Neighbors(0:00:00.004999)\n",
            "Training Extra Trees(0:00:04.773051)\n",
            "Training Random Forest (10 estimators)(0:00:01.510278)\n",
            "Training Gradient Boosting(0:02:39.048961)\n",
            "Training all classifiers on data took 0:03:29.829364\n",
            "\t>>> k: 1\n",
            "k took 0:00:02.587334\n",
            "\t>>> k: 2\n",
            "k took 0:00:33.517337\n",
            "\t>>> k: 3\n",
            "k took 0:03:16.569076\n",
            "\t>>> k: 4\n",
            "k took 0:13:42.194400\n",
            "\t>>> k: 5\n",
            "k took 0:36:26.092508\n",
            ">>> fold took: 0:57:30.812965\n",
            ">>> Fold 7\n",
            "Training Linear SVC(0:00:23.215055)\n",
            "Training Decision Tree(0:00:01.775029)\n",
            "Training Extra Tree(0:00:00.049039)\n",
            "Training Logistic(0:00:01.191947)\n",
            "Training Passive Aggressive(0:00:00.396998)\n",
            "Training Perceptron(0:00:00.302003)\n",
            "Training Ridge(0:00:00.029998)\n",
            "Training SGD(0:00:05.291998)\n",
            "Training Multi-layer Perceptron(0:00:06.669001)\n",
            "Training Linear Discriminant(0:00:00.227999)\n",
            "Training Quadratic Discriminant(0:00:00.062997)\n",
            "Training BernoulliNB(0:00:00.020004)\n",
            "Training Nearest Neighbors(0:00:00.003001)\n",
            "Training Extra Trees(0:00:04.721999)\n",
            "Training Random Forest (10 estimators)(0:00:01.529000)\n",
            "Training Gradient Boosting(0:02:41.531200)\n",
            "Training all classifiers on data took 0:03:27.023257\n",
            "\t>>> k: 1\n",
            "k took 0:00:02.603972\n",
            "\t>>> k: 2\n",
            "k took 0:00:31.379995\n",
            "\t>>> k: 3\n",
            "k took 0:03:15.409268\n",
            "\t>>> k: 4\n",
            "k took 0:12:25.714950\n",
            "\t>>> k: 5\n",
            "k took 0:35:57.894295\n",
            ">>> fold took: 0:55:40.037773\n",
            ">>> Fold 8\n",
            "Training Linear SVC(0:00:23.014013)\n",
            "Training Decision Tree(0:00:01.886030)\n",
            "Training Extra Tree(0:00:00.044997)\n",
            "Training Logistic(0:00:01.132997)\n",
            "Training Passive Aggressive(0:00:00.396992)\n",
            "Training Perceptron(0:00:00.325000)\n",
            "Training Ridge(0:00:00.026002)\n",
            "Training SGD(0:00:04.642996)\n",
            "Training Multi-layer Perceptron(0:00:07.811997)\n",
            "Training Linear Discriminant(0:00:00.156001)\n",
            "Training Quadratic Discriminant(0:00:00.043001)\n",
            "Training BernoulliNB(0:00:00.026999)\n",
            "Training Nearest Neighbors(0:00:00.006001)\n",
            "Training Extra Trees(0:00:04.047999)\n",
            "Training Random Forest (10 estimators)(0:00:01.372032)\n",
            "Training Gradient Boosting(0:02:33.198775)\n",
            "Training all classifiers on data took 0:03:18.134819\n",
            "\t>>> k: 1\n",
            "k took 0:00:02.449250\n",
            "\t>>> k: 2\n",
            "k took 0:00:32.224513\n",
            "\t>>> k: 3\n",
            "k took 0:03:29.467771\n",
            "\t>>> k: 4\n",
            "k took 0:13:39.916090\n",
            "\t>>> k: 5\n",
            "k took 0:40:19.707495\n",
            ">>> fold took: 1:01:21.912950\n",
            ">>> Fold 9\n",
            "Training Linear SVC(0:00:28.977045)\n",
            "Training Decision Tree(0:00:01.945086)\n",
            "Training Extra Tree(0:00:00.044382)\n",
            "Training Logistic(0:00:01.257996)\n",
            "Training Passive Aggressive(0:00:00.519636)\n",
            "Training Perceptron(0:00:00.348068)\n",
            "Training Ridge(0:00:00.028732)\n",
            "Training SGD(0:00:06.295276)\n",
            "Training Multi-layer Perceptron(0:00:12.407289)\n",
            "Training Linear Discriminant(0:00:00.185982)\n",
            "Training Quadratic Discriminant(0:00:00.050002)\n",
            "Training BernoulliNB(0:00:00.027000)\n",
            "Training Nearest Neighbors(0:00:00.004999)\n",
            "Training Extra Trees(0:00:05.377860)\n",
            "Training Random Forest (10 estimators)(0:00:01.690446)\n",
            "Training Gradient Boosting(0:03:04.862641)\n",
            "Training all classifiers on data took 0:04:04.027451\n",
            "\t>>> k: 1\n",
            "k took 0:00:02.541396\n",
            "\t>>> k: 2\n",
            "k took 0:00:30.600226\n",
            "\t>>> k: 3\n",
            "k took 0:03:09.036018\n",
            "\t>>> k: 4\n",
            "k took 0:12:48.727203\n",
            "\t>>> k: 5\n",
            "k took 0:40:57.285339\n",
            ">>> fold took: 1:01:32.226750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju1w2RGxk1lc"
      },
      "source": [
        "## Analysing results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scSXWuWTlEVx"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vVQX6L1lKVd"
      },
      "source": [
        "def saveResultsAsCsv(filename, headers, rows):\r\n",
        "  if len(headers) == len(rows[0]):\r\n",
        "    with open(filename,'w') as file:\r\n",
        "      file.write(','.join(headers))\r\n",
        "      file.write('\\n')\r\n",
        "      for row in rows:\r\n",
        "        file.write(','.join(list(map(lambda rc: str(rc), row))))\r\n",
        "        file.write('\\n')\r\n",
        "  else:\r\n",
        "    raise Exception('length of headers does not match length of single rows.')\r\n",
        "\r\n",
        "def filterResultsByDataset(datasetName, results):\r\n",
        "  return list(filter(lambda res: str(res[2]) == datasetName, results))\r\n",
        "\r\n",
        "def loadResultsFromCsv(filename):\r\n",
        "  loadedResults = []\r\n",
        "  with open(filename) as file:\r\n",
        "    loadedResults = list(map(lambda entry: entry.split(','), [x.strip() for x in file.readlines()]))\r\n",
        "  file.close()\r\n",
        "  print('Headers are:', loadedResults[0])\r\n",
        "  del loadedResults[0]\r\n",
        "  return loadedResults\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFYgGR4blnTo"
      },
      "source": [
        "### Aggregate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5gaTRHclGZo"
      },
      "source": [
        "results = list(map(lambda entry: (entry[0], entry[1], entry[2], entry[3], entry[4], entry[5].total_seconds()*1000), results))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93_jB0a7nEKs"
      },
      "source": [
        "resultsDf = pd.DataFrame(results, columns=['ensembleDescription', 'ensembleSize', 'foldIndex', 'dataset', 'errorRate', 'timeTaken'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxjQN8YbnFMb"
      },
      "source": [
        "finalResults = []\r\n",
        "\r\n",
        "# (ensemble description, ensemble size, fold index, dataset, score, timedelta)\r\n",
        "for ensemble_description, matchingResults in resultsDf.groupby(['ensembleDescription']):\r\n",
        "  matchingIterable = matchingResults.values\r\n",
        "  meanError = 0\r\n",
        "  meanTimeTaken = 0\r\n",
        "  for row in matchingIterable:\r\n",
        "    meanError += float(row[4])\r\n",
        "    meanTimeTaken += float(row[5])\r\n",
        "  meanError /= 10\r\n",
        "  meanTimeTaken /= 10\r\n",
        "  # (ensemble_description, ensemble_size, dataset, mean_error, time_taken)\r\n",
        "  finalResults.append((\r\n",
        "      ensemble_description, # matchingIterable[0][0]\r\n",
        "      matchingIterable[0][1], \r\n",
        "      matchingIterable[0][3], \r\n",
        "      meanError, \r\n",
        "      meanTimeTaken\r\n",
        "  ))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9QYgGRnPfgT"
      },
      "source": [
        "finalResults = loadResultsFromCsv('finalResults.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSMHbcE-rJRd"
      },
      "source": [
        "### Save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rikxkjMrOFX"
      },
      "source": [
        "saveResultsAsCsv('finalResultsJannis.csv', ['ensembleDescription', 'k', 'dataset', 'errorRate', 'timeTaken'], finalResults)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdnGpkoRT2tG"
      },
      "source": [
        "### Load results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA_J9oNNT4qH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3227ec6-8421-4165-f5bd-9a32499276b4"
      },
      "source": [
        "finalResults = loadResultsFromCsv('finalResultsJannis.csv')\r\n",
        "resultsDf = pd.read_csv('finalResultsJannis.csv')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Headers are: ['ensembleDescription', 'k', 'dataset', 'errorRate', 'timeTaken']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nL_M4UJqSog"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17hLSwlgqcQw"
      },
      "source": [
        "#### Error rates for different sizes of k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVysIdAcpB-Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "0dd7a9e8-b2cf-495e-8942-27af721001f1"
      },
      "source": [
        "for dataset, matchingResults in resultsDf.groupby(['dataset']): \r\n",
        "  boxplots = []\r\n",
        "  for k, matchingKs in matchingResults.groupby(['k']):\r\n",
        "    boxplots.append(matchingKs['errorRate'].values)\r\n",
        "  fig, ax = plt.subplots()\r\n",
        "  ax.boxplot(boxplots)\r\n",
        "  ax.set_title(dataset)\r\n",
        "  plt.show()  "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_54zvHabpJp0"
      },
      "source": [
        "#### Best single learners for every dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vTSoKttpN1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5418e9f-c002-4ee7-eb55-4024a47aa90e"
      },
      "source": [
        "bestSingleLearners = {} # dataset: (ensembleDescription, errorRate)\r\n",
        "\r\n",
        "for dataset, matchingResults in resultsDf.groupby(['dataset']): \r\n",
        "  matchingK1s = matchingResults.loc[resultsDf['k'] == 1]\r\n",
        "  minIndex = matchingK1s['errorRate'].idxmin()\r\n",
        "  minRow = resultsDf.iloc[minIndex]\r\n",
        "  bestSingleLearners[dataset] = (minRow['ensembleDescription'], minRow['errorRate'])\r\n",
        "\r\n",
        "bestSingleLearners"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'jannis': ('Gradient Boosting', 0.31464)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--KHaeYhq5lm"
      },
      "source": [
        "#### For every $k \\in \\{2,3,4,5\\}$, how many ensembles do improve over the single best learner by at least $0.01 (1\\%)$ (absolute and relative counts of improvement)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm-tsV5_lMwV"
      },
      "source": [
        "resultsDf[resultsDf['k'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXxF7SdyvCHd"
      },
      "source": [
        "improvementMargin = 0.01\r\n",
        "\r\n",
        "improvementEnsembles = {dataset: [] for dataset in bestSingleLearners.keys()} # dataset: (ensembleDescription, errorRate, k)\r\n",
        "\r\n",
        "for dataset, matchingResults in resultsDf.groupby(['dataset']): \r\n",
        "  for k, matchingKs in matchingResults.groupby(['k']):\r\n",
        "    if k != 1: # ...\r\n",
        "      for i, row in matchingKs.iterrows():\r\n",
        "        if (row['errorRate'] + improvementMargin) < bestSingleLearners[dataset][1]:\r\n",
        "          improvementEnsembles[dataset].append((row['ensembleDescription'], row['errorRate'], k))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhWhd6OyygLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e4d80be-0c87-455a-bfdc-39240d89667c"
      },
      "source": [
        "for dataset in improvementEnsembles.keys():\r\n",
        "  print(f\"\\n{dataset}'s best single learner had an error rate of: {bestSingleLearners[dataset][1]}\")\r\n",
        "  nImproved = len(improvementEnsembles[dataset])\r\n",
        "  print(f\"\\t{nImproved} ({round(nImproved/resultsDf[(resultsDf['dataset'] == dataset) & (resultsDf['k'] != 1)]['dataset'].count()*100, 2)}%) ensembles improved by at least {improvementMargin} ({improvementMargin*100}%)\")\r\n",
        "  for k in range(2, 6):\r\n",
        "    matchingImprovementEnsemblesWithK = list(filter(lambda entry: entry[2] == k, improvementEnsembles[dataset]))\r\n",
        "    print(f\"\\t\\t{len(matchingImprovementEnsemblesWithK)} with k={k} - mean error rate: {np.mean(list(map(lambda tup: tup[1], matchingImprovementEnsemblesWithK)))}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "jannis's best single learner had an error rate of: 0.31464\n",
            "\t0 (0.0%) ensembles improved by at least 0.01 (1.0%)\n",
            "\t\t0 with k=2 - mean error rate: nan\n",
            "\t\t0 with k=3 - mean error rate: nan\n",
            "\t\t0 with k=4 - mean error rate: nan\n",
            "\t\t0 with k=5 - mean error rate: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\aceve\\miniconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akz8ZeAD3TeB"
      },
      "source": [
        "#### For each base learner understand in how many improving ensembles it is contained (absolute and relative numbers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix4Xqxme34Ym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "507c7c2c-36c2-4407-b7db-03ccfb9744e3"
      },
      "source": [
        "for dataset in datasets.keys():\r\n",
        "  print(f'Dataset: {dataset}')\r\n",
        "  improvers = [] # classifier, timesAppearedInImprovements, timesAppearedInImprovementsRelative\r\n",
        "  for classifier in classifiers.keys():\r\n",
        "    timesAppearedInImprovements = 0\r\n",
        "    for row in improvementEnsembles[dataset]:\r\n",
        "      if classifier in row[0]:\r\n",
        "        timesAppearedInImprovements += 1\r\n",
        "    if len(improvementEnsembles[dataset]) != 0:\r\n",
        "      improvers.append((classifier, timesAppearedInImprovements, timesAppearedInImprovements/len(improvementEnsembles[dataset])))\r\n",
        "    else:\r\n",
        "      improvers.append((classifier, 0, 0))\r\n",
        "\r\n",
        "  for entry in sorted(improvers, key=lambda tup: tup[1], reverse=True):\r\n",
        "    print(f\"\\t{entry[0]}: {entry[1]} ({entry[2]*100}%)\")\r\n",
        "  print()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: jannis\n",
            "\tLinear SVC: 0 (0%)\n",
            "\tDecision Tree: 0 (0%)\n",
            "\tExtra Tree: 0 (0%)\n",
            "\tLogistic: 0 (0%)\n",
            "\tPassive Aggressive: 0 (0%)\n",
            "\tPerceptron: 0 (0%)\n",
            "\tRidge: 0 (0%)\n",
            "\tSGD: 0 (0%)\n",
            "\tMulti-layer Perceptron: 0 (0%)\n",
            "\tLinear Discriminant: 0 (0%)\n",
            "\tQuadratic Discriminant: 0 (0%)\n",
            "\tBernoulliNB: 0 (0%)\n",
            "\tNearest Neighbors: 0 (0%)\n",
            "\tExtra Trees: 0 (0%)\n",
            "\tRandom Forest (10 estimators): 0 (0%)\n",
            "\tGradient Boosting: 0 (0%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}