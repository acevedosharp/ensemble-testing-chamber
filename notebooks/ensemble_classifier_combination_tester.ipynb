{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ensemble-classifier-combination-tester.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNw1R2npjZohoKPUyjN/Jwu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acevedosharp/ensemble-testing-chamber/blob/master/notebooks/ensemble_classifier_combination_tester.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5POcuMGvFbr"
      },
      "source": [
        "# Measuring the effectiveness of ensembles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VVkWfGqHHs6"
      },
      "source": [
        "- assemble all possible combination of ensembles $k \\in \\{1,2,3,4,5\\}$\r\n",
        "- 10-fold cross validation\r\n",
        "- the single best learner (according to some cross-validation) for each dataset\r\n",
        "- for each $k \\in \\{2,3,4,5\\}$, how many ensembles do improve over the single best learner by at least $0.005$ (absolute and relative counts of improvement)\r\n",
        "- for each base learner understand in how many improving ensembles it is contained (absolute and relative numbers)\r\n",
        "- Considering all the ensembles of size k improving upon the single best solver by at least 0.01, how many learners (top l) do we need to consider to obtain a rate of 90% of the ensembles only containing learners in this set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCu4pVZQvAQq"
      },
      "source": [
        "## Experimental setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOTCnQy8CnV0"
      },
      "source": [
        "# ignore sklearn warnings\r\n",
        "def warn(*args, **kwargs):\r\n",
        "    pass\r\n",
        "import warnings\r\n",
        "warnings.warn = warn\r\n",
        "\r\n",
        "import itertools\r\n",
        "import random\r\n",
        "import openml\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import sklearn\r\n",
        "from datetime import datetime\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from functools import reduce\r\n",
        "\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.impute import KNNImputer\r\n",
        "\r\n",
        "# classifiers\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.tree import ExtraTreeClassifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\r\n",
        "from sklearn.linear_model import Perceptron\r\n",
        "from sklearn.linear_model import RidgeClassifier\r\n",
        "from sklearn.linear_model import SGDClassifier\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\r\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\r\n",
        "from sklearn.naive_bayes import BernoulliNB\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.ensemble import ExtraTreesClassifier\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.ensemble import GradientBoostingClassifier\r\n",
        "\r\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEHheHDUezaf"
      },
      "source": [
        "allDatasets = [\r\n",
        "  (1485 , 'madelon'),\r\n",
        "  # (42734, 'okcupid-stem'), requires too much preprocessing\r\n",
        "  (1169 , 'airlines'),\r\n",
        "  # (42733, 'Click_prediction_small'), after creating dummy columns it becomes way too large to fit on machines with less than 32gb of memory.\r\n",
        "  (41150, 'MiniBooNE'),\r\n",
        "  (54   , 'vehicle'), \r\n",
        "  (40981, 'Australian'),\r\n",
        "  (40975, 'car'),\r\n",
        "  (40670, 'dna'),\r\n",
        "  (31   , 'credit-g'),\r\n",
        "  (41169, 'helena'),\r\n",
        "  (41168, 'jannis'),\r\n",
        "  (41167, 'dionis'),\r\n",
        "  (41166, 'volkert'),\r\n",
        "  (41165, 'robert'),\r\n",
        "  (41164, 'fabert'),\r\n",
        "  (41163, 'dilbert'),\r\n",
        "  (41162, 'kick'),\r\n",
        "  (41161, 'riccardo'),\r\n",
        "  (41159, 'guillermo'),\r\n",
        "]\r\n",
        "\r\n",
        "indicesToRun = [9] # list(range(len(allDatasets)))\r\n",
        "rawDatasets = { allDatasets[i][1]: openml.datasets.get_dataset(allDatasets[i][0]) for i in indicesToRun } # download the datasets that will actually be used for the experiment\r\n",
        "\r\n",
        "datasets = { }\r\n",
        "\r\n",
        "for datasetName, rawDataset in rawDatasets.items():\r\n",
        "  X, Y, categorical_indicator, attribute_names = rawDatasets[datasetName].get_data(\r\n",
        "    dataset_format=\"dataframe\", target=rawDatasets[datasetName].default_target_attribute\r\n",
        "  )\r\n",
        "  if X.shape[0] > 25000: \r\n",
        "    arr = np.arange(0, X.shape[0])\r\n",
        "    random.shuffle(arr)\r\n",
        "    X = X.iloc[arr[:25000]]\r\n",
        "    Y = Y.iloc[arr[:25000]]\r\n",
        "  \r\n",
        "  X = pd.get_dummies(X).to_numpy()\r\n",
        "  Y = Y.to_numpy()\r\n",
        "  \r\n",
        "  datasets[datasetName] = (\r\n",
        "      X,\r\n",
        "      LabelEncoder().fit_transform(Y), # make sure all clases are integers\r\n",
        "      categorical_indicator\r\n",
        "  )\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAyRvsAVKPLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71592f78-de4b-4660-f6f7-d0cc66d3e2df"
      },
      "source": [
        "datasets.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['jannis'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7b6pFVrlyPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec5791a-2a5a-4c45-9e4c-b93960f225eb"
      },
      "source": [
        "datasets['jannis'][0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 54)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOeNKRHjdcR8"
      },
      "source": [
        "classifiers = {\r\n",
        "    \"Linear SVC\": LinearSVC(),\r\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\r\n",
        "    \"Extra Tree\": ExtraTreeClassifier(),\r\n",
        "    \"Logistic\": LogisticRegression(),\r\n",
        "    \"Passive Aggressive\": PassiveAggressiveClassifier(),\r\n",
        "    \"Perceptron\": Perceptron(),\r\n",
        "    \"Ridge\": RidgeClassifier(),\r\n",
        "    \"SGD\": SGDClassifier(),\r\n",
        "    \"Multi-layer Perceptron\": MLPClassifier(),\r\n",
        "    \"Linear Discriminant\": LinearDiscriminantAnalysis(),\r\n",
        "    \"Quadratic Discriminant\": QuadraticDiscriminantAnalysis(),\r\n",
        "    \"BernoulliNB\": BernoulliNB(),\r\n",
        "    # don't forget to comment it when dataset contains negative values\r\n",
        "    \"MultinomialNB\": MultinomialNB(),\r\n",
        "    \"Nearest Neighbors\": KNeighborsClassifier(),\r\n",
        "    \"Extra Trees\": ExtraTreesClassifier(),\r\n",
        "    \"Random Forest (10 estimators)\": RandomForestClassifier(n_estimators=10),\r\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier()\r\n",
        "}"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5YLMIyYkxk2"
      },
      "source": [
        "### Execute experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4ORrJknmQBF"
      },
      "source": [
        "# 10-fold cross validation, take only 25k instances for training in dataset if larger\r\n",
        "FOLDS = 10\r\n",
        "\r\n",
        "results = []\r\n",
        "singleClassifiersRuntimeAcrossDifferentFolds = [{classifier: (datetime.now() - datetime.now()) for classifier in classifiers.keys()} for i in range(FOLDS)]\r\n",
        "\r\n",
        "kf = KFold(n_splits=FOLDS)\r\n",
        "for dataset in datasets.keys():\r\n",
        "  X, Y = datasets[dataset][0], datasets[dataset][1]\r\n",
        "  #X = StandardScaler().fit_transform(X)\r\n",
        "  fold_index = 0\r\n",
        "  for train_index, test_index in kf.split(X):\r\n",
        "    fold_start_time = datetime.now()\r\n",
        "    print(\">>> Fold\", fold_index)\r\n",
        "    X_train, X_test = X[train_index], X[test_index] # np.take\r\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index] # np.take\r\n",
        "\r\n",
        "    # Train every classifier with the new data\r\n",
        "    trainStartTime = datetime.now()\r\n",
        "    for classifier_name, classifier in classifiers.items():\r\n",
        "      print(f\"Training {classifier_name}\", end='')\r\n",
        "      classifierStartTime = datetime.now()\r\n",
        "      classifier.fit(X_train, Y_train)\r\n",
        "      delta = datetime.now() - classifierStartTime\r\n",
        "      print(f'({datetime.now() - classifierStartTime})')\r\n",
        "      singleClassifiersRuntimeAcrossDifferentFolds[fold_index][classifier_name] = delta\r\n",
        "    print(f'Training all classifiers on data took {datetime.now() - trainStartTime}')\r\n",
        "\r\n",
        "    # Assemble ensembles of size k in {1,2,3,4,5}\r\n",
        "    for k in range(1,6):\r\n",
        "      print(\"\\t>>> k:\", k)\r\n",
        "      kStartTime = datetime.now()\r\n",
        "      for combination in list(itertools.combinations(classifiers.keys(), k)):\r\n",
        "        singleEnsembleStartTime = datetime.now()\r\n",
        "        ensemble = []\r\n",
        "        ensemble_description = \"\"\r\n",
        "\r\n",
        "        # group classifiers (already exist fitted in dict)\r\n",
        "        for idx in range(k):\r\n",
        "          ensemble.append(classifiers[combination[idx]])\r\n",
        "          ensemble_description += combination[idx]\r\n",
        "          ensemble_description += \"-\"\r\n",
        "        ensemble_description = ensemble_description[:-1]\r\n",
        "\r\n",
        "        # save predictions\r\n",
        "        predictions = np.zeros((len(X_test), k)) # (# test instances, ensemble size)\r\n",
        "        for idx, classifier in enumerate(ensemble):\r\n",
        "          predictions[:,idx] = classifier.predict(X_test)\r\n",
        "        \r\n",
        "        # do hard voting\r\n",
        "        hard_voting_predictions = np.zeros((len(X_test), 1)) # (# test instances, 1)\r\n",
        "        for idx in range(predictions.shape[0]):\r\n",
        "          values, counts = np.unique(predictions[idx], return_counts=True)\r\n",
        "          hard_voting_predictions[idx] = values[np.argmax(counts)]\r\n",
        "        \r\n",
        "        # compare voting predictions against Y_test\r\n",
        "        total_instance_number = len(X_test)\r\n",
        "        errors = 0\r\n",
        "        for idx in range(hard_voting_predictions.shape[0]):\r\n",
        "          if (hard_voting_predictions[idx][0] != Y_test[idx]):\r\n",
        "            errors += 1\r\n",
        "        score = errors/total_instance_number\r\n",
        "\r\n",
        "        # save result (ensemble description, ensemble size, fold index, dataset, score, timedelta)\r\n",
        "        results.append((ensemble_description, k, fold_index, dataset, score, datetime.now() - singleEnsembleStartTime))\r\n",
        "      print(f\"k took {datetime.now() - kStartTime}\")\r\n",
        "\r\n",
        "    fold_index += 1\r\n",
        "    print(\">>> fold took:\", datetime.now() - fold_start_time)\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju1w2RGxk1lc"
      },
      "source": [
        "## Analysing results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scSXWuWTlEVx"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vVQX6L1lKVd"
      },
      "source": [
        "def saveResultsAsCsv(filename, headers, rows):\r\n",
        "  if len(headers) == len(rows[0]):\r\n",
        "    with open(filename,'w') as file:\r\n",
        "      file.write(','.join(headers))\r\n",
        "      file.write('\\n')\r\n",
        "      for row in rows:\r\n",
        "        file.write(','.join(list(map(lambda rc: str(rc), row))))\r\n",
        "        file.write('\\n')\r\n",
        "  else:\r\n",
        "    raise Exception('length of headers does not match length of single rows.')\r\n",
        "\r\n",
        "def filterResultsByDataset(datasetName, results):\r\n",
        "  return list(filter(lambda res: str(res[2]) == datasetName, results))\r\n",
        "\r\n",
        "def loadResultsFromCsv(filename):\r\n",
        "  loadedResults = []\r\n",
        "  with open(filename) as file:\r\n",
        "    loadedResults = list(map(lambda entry: entry.split(','), [x.strip() for x in file.readlines()]))\r\n",
        "  file.close()\r\n",
        "  print('Headers are:', loadedResults[0])\r\n",
        "  del loadedResults[0]\r\n",
        "  return loadedResults\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFYgGR4blnTo"
      },
      "source": [
        "### Aggregate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5gaTRHclGZo"
      },
      "source": [
        "results = list(map(lambda entry: (entry[0], entry[1], entry[2], entry[3], entry[4], entry[5].total_seconds()*1000), results))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93_jB0a7nEKs"
      },
      "source": [
        "resultsDf = pd.DataFrame(results, columns=['ensembleDescription', 'ensembleSize', 'foldIndex', 'dataset', 'errorRate', 'timeTaken'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxjQN8YbnFMb"
      },
      "source": [
        "finalResults = []\r\n",
        "\r\n",
        "# (ensemble description, ensemble size, fold index, dataset, score, timedelta)\r\n",
        "for ensemble_description, matchingResults in resultsDf.groupby(['ensembleDescription']):\r\n",
        "  matchingIterable = matchingResults.values\r\n",
        "  meanError = 0\r\n",
        "  meanTimeTaken = 0\r\n",
        "  for row in matchingIterable:\r\n",
        "    meanError += float(row[4])\r\n",
        "    meanTimeTaken += float(row[5])\r\n",
        "  meanError /= 10\r\n",
        "  meanTimeTaken /= 10\r\n",
        "  # (ensemble_description, ensemble_size, dataset, mean_error, time_taken)\r\n",
        "  finalResults.append((\r\n",
        "      ensemble_description, # matchingIterable[0][0]\r\n",
        "      matchingIterable[0][1], \r\n",
        "      matchingIterable[0][3], \r\n",
        "      meanError, \r\n",
        "      meanTimeTaken\r\n",
        "  ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_p4VNQhlYhq"
      },
      "source": [
        "### Join multiple csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_6jot4alcP7"
      },
      "source": [
        "def join(names):\r\n",
        "  dataframes = []\r\n",
        "  for name in names:\r\n",
        "    dataframes.append(pd.read_csv(name))\r\n",
        "  return pd.concat(dataframes)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3RNkP9ql6wH"
      },
      "source": [
        "names = [\r\n",
        "         'results/finalResultsAirlines.csv',\r\n",
        "         'results/finalResultsAustralian.csv',\r\n",
        "         'results/finalResultsCar.csv',\r\n",
        "         'results/finalResultsCreditG.csv',\r\n",
        "         'results/finalResultsDna.csv',\r\n",
        "         'results/finalResultsHelena-8of10Folds.csv',\r\n",
        "         'results/finalResultsJannis.csv',\r\n",
        "         'results/finalResultsMadelon.csv',\r\n",
        "         'results/finalResultsMiniBooNE.csv',\r\n",
        "         'results/finalResultsVehicle.csv'\r\n",
        "]\r\n",
        "\r\n",
        "finalResults = join(names)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9QYgGRnPfgT"
      },
      "source": [
        "finalResults = loadResultsFromCsv('finalResults.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSMHbcE-rJRd"
      },
      "source": [
        "### Save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rikxkjMrOFX"
      },
      "source": [
        "saveResultsAsCsv('results/finalResults.csv', ['ensembleDescription', 'k', 'dataset', 'errorRate', 'timeTaken'], finalResults.to_numpy())"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdnGpkoRT2tG"
      },
      "source": [
        "### Load results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA_J9oNNT4qH"
      },
      "source": [
        "finalResults = loadResultsFromCsv('results/finalResults.csv')\r\n",
        "resultsDf = pd.read_csv('results/finalResults.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nL_M4UJqSog"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17hLSwlgqcQw"
      },
      "source": [
        "#### Error rates for different sizes of k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVysIdAcpB-Y"
      },
      "source": [
        "for dataset, matchingResults in resultsDf.groupby(['dataset']): \r\n",
        "  boxplots = []\r\n",
        "  for k, matchingKs in matchingResults.groupby(['k']):\r\n",
        "    boxplots.append(matchingKs['errorRate'].values)\r\n",
        "  fig, ax = plt.subplots()\r\n",
        "  ax.boxplot(boxplots)\r\n",
        "  ax.set_title(dataset)\r\n",
        "  plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_54zvHabpJp0"
      },
      "source": [
        "#### Best single learners for every dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vTSoKttpN1r"
      },
      "source": [
        "bestSingleLearners = {} # dataset: (ensembleDescription, errorRate)\r\n",
        "\r\n",
        "for dataset, matchingResults in resultsDf.groupby(['dataset']): \r\n",
        "  matchingK1s = matchingResults.loc[resultsDf['k'] == 1]\r\n",
        "  minIndex = matchingK1s['errorRate'].idxmin()\r\n",
        "  minRow = resultsDf.iloc[minIndex]\r\n",
        "  bestSingleLearners[dataset] = (minRow['ensembleDescription'], minRow['errorRate'])\r\n",
        "\r\n",
        "bestSingleLearners"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--KHaeYhq5lm"
      },
      "source": [
        "#### For every $k \\in \\{2,3,4,5\\}$, how many ensembles do improve over the single best learner by at least $0.01 (1\\%)$ (absolute and relative counts of improvement)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXxF7SdyvCHd"
      },
      "source": [
        "improvementMargin = 0.01\r\n",
        "\r\n",
        "improvementEnsembles = {dataset: [] for dataset in bestSingleLearners.keys()} # dataset: (ensembleDescription, errorRate, k)\r\n",
        "\r\n",
        "for dataset, matchingResults in resultsDf.groupby(['dataset']): \r\n",
        "  for k, matchingKs in matchingResults.groupby(['k']):\r\n",
        "    if k != 1: # ...\r\n",
        "      for i, row in matchingKs.iterrows():\r\n",
        "        if (row['errorRate'] + improvementMargin) < bestSingleLearners[dataset][1]:\r\n",
        "          improvementEnsembles[dataset].append((row['ensembleDescription'], row['errorRate'], k))"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhWhd6OyygLb"
      },
      "source": [
        "for dataset in improvementEnsembles.keys():\r\n",
        "  print(f\"\\n{dataset}'s best single learner had an error rate of: {bestSingleLearners[dataset][1]}\")\r\n",
        "  nImproved = len(improvementEnsembles[dataset])\r\n",
        "  print(f\"\\t{nImproved} ({round(nImproved/resultsDf[(resultsDf['dataset'] == dataset) & (resultsDf['k'] != 1)]['dataset'].count()*100, 2)}%) ensembles improved by at least {improvementMargin} ({improvementMargin*100}%)\")\r\n",
        "  for k in range(2, 6):\r\n",
        "    matchingImprovementEnsemblesWithK = list(filter(lambda entry: entry[2] == k, improvementEnsembles[dataset]))\r\n",
        "    print(f\"\\t\\t{len(matchingImprovementEnsemblesWithK)} with k={k} - mean error rate: {np.mean(list(map(lambda tup: tup[1], matchingImprovementEnsemblesWithK)))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akz8ZeAD3TeB"
      },
      "source": [
        "#### For each base learner understand in how many improving ensembles it is contained (absolute and relative numbers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix4Xqxme34Ym"
      },
      "source": [
        "actualImprovers = {}\r\n",
        "\r\n",
        "for dataset, matchingResults in resultsDf.groupby(['dataset']):\r\n",
        "  print(f'Dataset: {dataset}')\r\n",
        "  improvers = [] # classifier, timesAppearedInImprovements, timesAppearedInImprovementsRelative\r\n",
        "  for classifier in classifiers.keys():\r\n",
        "    timesAppearedInImprovements = 0\r\n",
        "    for row in improvementEnsembles[dataset]:\r\n",
        "      if classifier in row[0]:\r\n",
        "        timesAppearedInImprovements += 1\r\n",
        "    if len(improvementEnsembles[dataset]) != 0:\r\n",
        "      improvers.append((classifier, timesAppearedInImprovements, timesAppearedInImprovements/len(improvementEnsembles[dataset])))\r\n",
        "    else:\r\n",
        "      improvers.append((classifier, 0, 0))\r\n",
        "\r\n",
        "  for i, entry in enumerate(sorted(improvers, key=lambda tup: tup[1], reverse=True)):\r\n",
        "    if (entry[1] == 0 and i==0): #If \"best\" classifier had 0 appearances, none improved.\r\n",
        "      print('\\tNone improved')\r\n",
        "      break\r\n",
        "    if (dataset in actualImprovers):\r\n",
        "      actualImprovers[dataset].append(entry)\r\n",
        "    else:\r\n",
        "      actualImprovers[dataset] = [entry]\r\n",
        "    print(f\"\\t{entry[0]}: {entry[1]} ({entry[2]*100}%)\")\r\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-skbKCTwqwIv"
      },
      "source": [
        "#### Considering all the ensembles of size k improving upon the single best solver by at least $0.01$ ($1\\%$), how many learners (top $l$) do we need to consider to obtain a rate of $90\\%$ of the ensembles only containing learners in this set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3kxGDnhsPHa"
      },
      "source": [
        "targetRate = 0.9\r\n",
        "\r\n",
        "for dataset, matchingResults in resultsDf.groupby(['dataset']):\r\n",
        "  print(f\"dataset: {dataset}\")\r\n",
        "  # subset of improving ensemble for this dataset\r\n",
        "  improversForDataset = matchingResults[matchingResults['errorRate'] + improvementMargin < bestSingleLearners[dataset][1]]\r\n",
        "  for k in range(1, 6):\r\n",
        "    print(f\"\\tk: {k}\")\r\n",
        "\r\n",
        "    # find the subset of improving ensembles for this particular k\r\n",
        "    improversForK = improversForDataset[improversForDataset['k'] == k]\r\n",
        "\r\n",
        "    if len(improversForK) == 0:\r\n",
        "      print(\"\\t\\tNo improvers\")\r\n",
        "    else:\r\n",
        "      # count how many times each single learner appeared within k improvers\r\n",
        "      timesSingleLearnersAppeared = { c: 0 for c in classifiers.keys()}\r\n",
        "      for classifier in classifiers.keys():\r\n",
        "        selfCount = 0\r\n",
        "        for i, row in improversForK.iterrows():\r\n",
        "          if classifier in row['ensembleDescription']:\r\n",
        "            selfCount += 1\r\n",
        "        timesSingleLearnersAppeared[classifier] = selfCount\r\n",
        "\r\n",
        "      # sort single learner appearances\r\n",
        "      sortedAppearances = {k: v for k, v in sorted(timesSingleLearnersAppeared.items(), key=lambda item: item[1], reverse=True)}\r\n",
        "      print(sortedAppearances)\r\n",
        "\r\n",
        "      # try different values of l\r\n",
        "      chosenL = 0\r\n",
        "      previousRate = 1\r\n",
        "      for l in range(len(classifiers)-1, -1, -1):\r\n",
        "        classifiersLSubset = list(sortedAppearances.keys())[:l]\r\n",
        "        ensembleCounts = 0\r\n",
        "        for i, row in improversForK.iterrows():\r\n",
        "          classifiersWithinEnsemble = row['ensembleDescription'].split('-')\r\n",
        "          countSatisfied = 0\r\n",
        "          for classifierWithinEnsemble in classifiersWithinEnsemble:\r\n",
        "            if classifierWithinEnsemble in classifiersLSubset:\r\n",
        "              countSatisfied += 1\r\n",
        "          if countSatisfied == len(classifiersWithinEnsemble):\r\n",
        "            ensembleCounts += 1\r\n",
        "        \r\n",
        "        resultingRate = ensembleCounts/len(improversForK)\r\n",
        "        print(f\"\\t\\t{l} {resultingRate}\")\r\n",
        "        if (resultingRate <= previousRate) and resultingRate >= targetRate:\r\n",
        "          chosenL = l\r\n",
        "      print(f\"\\t\\tl: {chosenL}\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}